# Prevent fluentd from handling records containing its own logs. Otherwise
# it can lead to an infinite loop, when error in sending one message generates
# another message which also fails to be sent and so on.
<match fluent.**>
  type null
</match>

<source>
  type tail
  format json
  time_key time
  path /var/log/containers/*.log
  pos_file /var/log/containers.log.pos
  time_format %Y-%m-%dT%H:%M:%S.%N%Z
  read_from_head true
  tag raw.kubernetes.*
</source>

# Detect exceptions in the log output and forward them as one log entry.
<match raw.kubernetes.**>
  type detect_exceptions_with_error
  remove_tag_prefix raw
  message log
  stream stream
  multiline_flush_interval 5
  max_bytes 500000
</match>

# Logs from systemd-journal for interesting services.
<source>
  type systemd
  filters [{ "_SYSTEMD_UNIT": "docker.service" }]
  pos_file /var/log/journald-docker.pos
  read_from_head true
  tag docker
</source>

<source>
  type systemd
  filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
  pos_file /var/log/journald-kubelet.pos
  read_from_head true
  tag kubelet
</source>

<filter kubernetes.**>
  type kubernetes_metadata
</filter>

<match **>
  type sumologic
  endpoint "#{ENV['SUMOLOGIC_URL']}"
  log_format json
  # Set the buffer type to file to improve the reliability and reduce the memory consumption
  buffer_type file
  buffer_path /var/log/fluentd-buffers/kubernetes.containers.buffer
  # Set queue_full action to block because we want to pause gracefully
  # in case of the off-the-limits load instead of throwing an exception
  buffer_queue_full_action block
  # Set the chunk limit conservatively to avoid exceeding the GCL limit
  # of 10MiB per write request.
  buffer_chunk_limit 2M
  # Cap the combined memory usage of this buffer and the one below to
  # 2MiB/chunk * (6 + 2) chunks = 16 MiB
  buffer_queue_limit 6
  # Never wait more than 5 seconds before flushing logs in the non-error case.
  flush_interval 5s
  # Never wait longer than 30 seconds between retries.
  max_retry_wait 30
  # Disable the limit on the number of retries (retry forever).
  disable_retry_limit
  # Use multiple threads for processing.
  num_threads 4
</match>
